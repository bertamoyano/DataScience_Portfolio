```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# Anàlisi textual

```{r}
library(readxl)
finalDataset0_2 <- read_excel("finalDataset0.2.xlsx")
```

```{r}
df<- data.frame(finalDataset0_2$coursecontent...4)
```

```{r}
# install.packages("tidytext")
# install.packages("textdata")
# install.packages("tm")
# install.packages("SnowballC")
# install.packages("rJava")
# install.packages("wordcloud")
# install.packages("janeaustenr")
# install.packages("lda")
# install.packages("ldatuning")
# install.packages("topicmodels")
library(janeaustenr)
library(dplyr)
library(stringr)
library(textdata)
library(tidytext)
library(wordcloud)
library(SnowballC)
# library(rJava)
library(tm)
library(dplyr)
library(tm)
library(lda)
library(ldatuning)
library(topicmodels)
```

## Preprocessament

Per poder desenvolupar l'anàlisi textual primer cal preprocessar la base de dades amb l'objectiu de mantenir només aquella informació rellevant que permeti una anàlisi més precisa.

Aquest preprocessament inclou eliminar els espais en blanc, les majúscules, els signes de puntuació, els números, altres caràcters especials, les stop words (to, the, a...), etc. A més, s'aplica el procés de lematització de les paraules (transformar les paraules a la seva arrel per poder trobar més relacions entre textos diferents).

```{r}
# create corpus
review_corpus<- VCorpus(VectorSource(df))
## Step 1: Eliminating extra whitespace
review_corpus<- tm_map(review_corpus, stripWhitespace)
## Step 2: Transform to lowercase
review_corpus<- tm_map(review_corpus, content_transformer(tolower))
## remove punctuation
review_corpus<- tm_map(review_corpus, removePunctuation)
## remove numbers
review_corpus<- tm_map(review_corpus, removeNumbers)
## create custom function to remove other misc characters
text_preprocessing<- function(x)
{gsub('http\\S+\\s*','',x) # remove URLs
  gsub('#\\S+','',x) # remove hashtags
  gsub('[[:cntrl:]]','',x) # remove controls and special characters
  gsub("^[[:space:]]*","",x) # remove leading whitespaces
  gsub("[[:space:]]*$","",x) # remove trailing whitespaces
  gsub(' +', ' ', x) # remove extra whitespaces
}
review_corpus<-tm_map(review_corpus,text_preprocessing)
# remove stopwords
review_corpus<- tm_map(review_corpus, removeWords, stopwords("english"))
# you can also create custom stopwords based on your context
mystopwords<- c(stopwords("english"),"course","courses")
review_corpus<- tm_map(review_corpus, removeWords, mystopwords)
review_corpus<- tm_map(review_corpus, stripWhitespace)

library(textstem)
review_corpus<- tm_map(review_corpus, PlainTextDocument)
review_corpus<- tm_map(review_corpus, content_transformer(lemmatize_words))
```

S'agafen les paraules que surten 5 vegades o més per tenir aquella informació rellevant i les representem gràficament:

```{r}
# create term document matrix
tdm<- TermDocumentMatrix(review_corpus, control = list(wordlengths = c(1,Inf),  weighting = weightTf))
freq_terms<- findFreqTerms(tdm, lowfreq=5) #només agafem les paraules que apareixen 5 o mes vegades
term_freq<- rowSums(as.matrix(tdm))
term_freq<- subset(term_freq, term_freq>=5)
data<- data.frame(term = names(term_freq), freq = term_freq)

# plot word frequency
library(ggplot2)
data_plot<- data %>%
  top_n(25)
# Plot word frequency
ggplot(data_plot, aes(x = reorder(term, +freq), y = freq, fill = freq)) + geom_bar(stat = "identity")+ scale_colour_gradientn(colors = terrain.colors(10))+ xlab("Terms")+ ylab("Count")+coord_flip()
# create word cloud
# install.packages("wordcloud2")
library(wordcloud2)

wordcloud2(data, color = "random-dark", backgroundColor = "white", shape="circle")
```

## Topic modeling

```{r, echo=FALSE, include=FALSE}
# #manera 1
# result <- ldatuning::FindTopicsNumber(
#   tdm,
#   topics = seq(from = 2, to = 20, by = 1),
#   metrics = c("CaoJuan2009",  "Deveaud2014"),
#   method = "Gibbs",
#   control = list(seed = 77),
#   verbose = TRUE
# )
# FindTopicsNumber_plot(result)
# 
# #manera 2
# # Build LDA Model
# data_lda <- LDA(data, k = 2, control = list(seed = 1234))
# data_lda
# as.data.frame(terms(data_lda, 15))
# 
# #manera 3
# burnin <-1000
# #set iterations
# iter<-2000
# #thin the spaces between samples
# thin <- 500
# #set random starts at 5
# nstart <-5
# #use random integers as seed 
# seed <- list(254672,109,122887,145629037,2)
# # return the highest probability as the result
# best <-TRUE
# #set number of topics 
# k <-5
# #run the LDA model
# ldaOut <- LDA(tdm,k, method="Gibbs", control=
#                 list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
# terms(ldaOut,6)
# https://bookdown.org/tianyuan09/ai4ph2022/tutorial2.html
```

```{r, echo = FALSE, include=FALSE}
# intent 4
library(tidyverse)
library(readr)
# a collection of package for data wrangling.
library(tidyverse)
# package for text processing
library(tidytext)
# collection of packages for modeling and L 

library(scales)
library(quanteda)
library(SnowballC)
library(topicmodels)
# install.packages("textrecipes")
library(textrecipes)

# install.packages("themis")
library(themis)

# install.packages("discrim")
library(discrim)

# install.packages("vip")
library(vip)

word_counts_dtm = data %>% count(freq, term) %>% cast_dfm(freq, term, n)

topfeatures(word_counts_dtm, n = 20, scheme = "docfreq")

lda2 <- LDA(word_counts_dtm, k = 2, control = list(seed = 1234))
lda2
as.data.frame(terms(lda2, 15))

lda4 <- LDA(word_counts_dtm, k = 4, control = list(seed = 1234))
lda4
as.data.frame(terms(lda4, 15))

lda_topics <- tidy(lda2, matrix = "beta")

lda_topics

lda_topics4 <-tidy(lda4,matrix="beta")
lda_topics4
```

### Per dos tòpics

Dividint les respostes dels alumnes en dos temes:

```{r, out.width="90%"}
# terms that are more common with each topic 
ap_top_terms <- lda_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()+
  labs(title = "The terms that are most common within each topic")
```

```{r, out.width="90%"}
# terms that generate the greatest differences in beta btween two topics 
beta_wide <- lda_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_wide %>% arrange(desc(abs(log_ratio)))%>% head(20) %>% 
  arrange(desc(log_ratio)) %>%
  ggplot(aes(log_ratio, term)) +
  geom_col(show.legend = FALSE)+
  labs(title = "Terms with the great difference in beta between two topics")

lda_documents <- tidy(lda2, matrix = "gamma")
```

### Per quatre tòpics

Dividint les respostes dels alumnes en quatre grups:

```{r, out.width="90%"}
# for 4 topics 
ap_top_terms4 <- lda_topics4 %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms4 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()+
  labs(title = "The terms that are most common within each topic")
```

En el gràfic anterior es pot observar que el primer tema tracta els coneixements obtinguts a l'assignatura. El segon engloba els continguts i els materials del curs. El tercer està format sobretot per sentiments i opinions dels alumnes, mentre que el quart tema són la universitat i els professors. 

Cal tenir en compte que hi ha un nombre limitat de respostes i aquestes són poc diverses, el que implica que l'anàlisi de temes no és del tot significatiu. 
